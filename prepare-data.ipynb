{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72b7a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "assert data_dir.exists(), f\"Data directory does not exist: {data_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8bff20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from copy import copy\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "from rich.progress import track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c942c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "first = json.loads(\n",
    "    (data_dir / \"Versuch_V4_T2_A1_Dec-18-25__19_26.json\").read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad32fc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dc98a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec5c90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0][\"meta_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42aefac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0][\"expert_knowledge\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cd52d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0][\"data\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d62364c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simpler = copy(first[0][\"data\"])\n",
    "del simpler[\"time_s\"]\n",
    "del simpler[\"signals\"]\n",
    "simpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9363e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.array(first[0][\"data\"][\"time_s\"])\n",
    "t.shape, [t.min().item(), t.mean().item(), t.max().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518f599e",
   "metadata": {},
   "outputs": [],
   "source": [
    "first[0][\"data\"][\"signals\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aaa84df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = np.array(first[0][\"data\"][\"signals\"][\"K1_Ch1_Mod2__AI0\"])\n",
    "s.shape, [s.min().item(), s.mean().item(), s.max().item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f102100d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = []\n",
    "\n",
    "# We iterate over all files in the data directory and collect all data\n",
    "for file_path in track(list(data_dir.glob(\"Versuch_*.json\"))):\n",
    "    data = json.loads(file_path.read_text())\n",
    "\n",
    "    for i, data_entry in enumerate(data):\n",
    "        agg.append({\n",
    "            \"meta_data\": {\n",
    "                \"Versuch\": file_path.stem,\n",
    "                \"index\": i,\n",
    "                \"V\": data_entry[\"meta_data\"][\"V\"],\n",
    "                \"T\": data_entry[\"meta_data\"][\"T\"],\n",
    "                \"A\": data_entry[\"meta_data\"][\"A\"],\n",
    "            },\n",
    "            \"data\": {\n",
    "                key: np.array(value)\n",
    "                for key, value in {\n",
    "                    \"time_s\": data_entry[\"data\"][\"time_s\"],\n",
    "                    **data_entry[\"data\"][\"signals\"]\n",
    "                }.items()\n",
    "            }\n",
    "        })\n",
    "\n",
    "# Get signal keys from first entry\n",
    "signal_keys = list(agg[0][\"data\"].keys())\n",
    "n_entries = len(agg)\n",
    "n_points = len(agg[0][\"data\"][\"time_s\"])\n",
    "\n",
    "print(f\"Number of entries: {n_entries}\")\n",
    "print(f\"Number of time points per entry: {n_points}\")\n",
    "print(f\"Number of signals: {len(signal_keys)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9eb127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all data into single arrays\n",
    "# Shape: (n_entries, n_signals, n_points)\n",
    "data_array = np.stack([\n",
    "    np.stack([entry[\"data\"][key] for key in signal_keys])\n",
    "    for entry in agg\n",
    "])\n",
    "\n",
    "# Metadata arrays\n",
    "versuch_array = np.array([entry[\"meta_data\"][\"Versuch\"]\n",
    "                         for entry in agg], dtype=\"S64\")\n",
    "index_array = np.array([entry[\"meta_data\"][\"index\"]\n",
    "                       for entry in agg], dtype=np.int32)\n",
    "v_array = np.array([entry[\"meta_data\"][\"V\"] for entry in agg], dtype=np.int32)\n",
    "t_array = np.array([entry[\"meta_data\"][\"T\"] for entry in agg], dtype=np.int32)\n",
    "a_array = np.array([entry[\"meta_data\"][\"A\"] for entry in agg], dtype=np.int32)\n",
    "\n",
    "print(f\"Data array shape: {data_array.shape}\")\n",
    "\n",
    "# Dump that into an HDF5 file for easier access later\n",
    "with h5py.File(data_dir / \"all_data.hdf5\", \"w\") as h5f:\n",
    "    # Store signal data as single dataset\n",
    "    h5f.create_dataset(\"data\", data=data_array, compression=\"gzip\")\n",
    "\n",
    "    # Store signal names as attribute\n",
    "    h5f[\"data\"].attrs[\"signal_keys\"] = [k.encode() for k in signal_keys]\n",
    "\n",
    "    # Store metadata as datasets\n",
    "    meta_grp = h5f.create_group(\"meta_data\")\n",
    "    meta_grp.create_dataset(\"Versuch\", data=versuch_array)\n",
    "    meta_grp.create_dataset(\"index\", data=index_array)\n",
    "    meta_grp.create_dataset(\"V\", data=v_array)\n",
    "    meta_grp.create_dataset(\"T\", data=t_array)\n",
    "    meta_grp.create_dataset(\"A\", data=a_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the file size\n",
    "h5_file_size = (data_dir / \"all_data.hdf5\").stat().st_size / (1024 ** 2)\n",
    "print(f\"HDF5 file size: {h5_file_size:.2f} MB\")\n",
    "\n",
    "# Verify that we can read back the data correctly\n",
    "with h5py.File(data_dir / \"all_data.hdf5\", \"r\") as h5f:\n",
    "    read_data = h5f[\"data\"][:]\n",
    "    read_signal_keys = [k for k in h5f[\"data\"].attrs[\"signal_keys\"]]\n",
    "\n",
    "    assert np.array_equal(data_array, read_data), \"Data mismatch\"\n",
    "    assert read_signal_keys == signal_keys, \"Signal keys mismatch\"\n",
    "    assert np.array_equal(h5f[\"meta_data/Versuch\"][:],\n",
    "                          versuch_array), \"Versuch mismatch\"\n",
    "    assert np.array_equal(h5f[\"meta_data/V\"][:], v_array), \"V mismatch\"\n",
    "    assert np.array_equal(h5f[\"meta_data/T\"][:], t_array), \"T mismatch\"\n",
    "    assert np.array_equal(h5f[\"meta_data/A\"][:], a_array), \"A mismatch\"\n",
    "\n",
    "del agg, data_array  # Free memory\n",
    "\n",
    "print(\"HDF5 file verified successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8fe78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
