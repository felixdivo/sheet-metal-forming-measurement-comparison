{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Experiment Results Visualization\n",
    "\n",
    "This notebook parses all experiment results from the `experiment_results` directory,\n",
    "combines them into a CSV file, and visualizes them as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from cmap import Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "channel_mapping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load channel mapping from central config file\n",
    "import json as _json\n",
    "with open(\"channel_config.json\", \"r\") as f:\n",
    "    _config = _json.load(f)\n",
    "    CHANNEL_MAPPING = _config[\"channel_mapping\"]\n",
    "    DIRECT_CHANNELS = _config[\"channel_groups\"][\"DIRECT\"]\n",
    "    INDIRECT_CHANNELS = _config[\"channel_groups\"][\"INDIRECT\"]\n",
    "    ALL_CHANNELS = _config[\"channel_groups\"][\"ALL\"]\n",
    "\n",
    "# Create index-to-name mapping (indices match the order in CHANNEL_MAPPING.keys())\n",
    "CHANNEL_KEYS = list(CHANNEL_MAPPING.keys())\n",
    "\n",
    "# Define canonical signal order based on channel_config.json\n",
    "# Single channels first (in config order), then groups\n",
    "SINGLE_CHANNEL_ORDER = [1, 2, 3, 4, 5, 7, 8, 9]  # Matches channel indices used in experiments\n",
    "GROUP_ORDER = [\"DIRECT\", \"INDIRECT\", \"ALL\"]\n",
    "\n",
    "def get_channel_name(channel_idx):\n",
    "    \"\"\"Get descriptive name for a channel index.\"\"\"\n",
    "    if 0 <= channel_idx < len(CHANNEL_KEYS):\n",
    "        return CHANNEL_MAPPING[CHANNEL_KEYS[channel_idx]]\n",
    "    return f\"Channel {channel_idx}\"\n",
    "\n",
    "def get_channels_label(channel_indices):\n",
    "    \"\"\"Get a readable label for a list of channel indices.\"\"\"\n",
    "    # Check for predefined groups first\n",
    "    if channel_indices == DIRECT_CHANNELS:\n",
    "        return \"DIRECT (3 channels)\"\n",
    "    elif channel_indices == INDIRECT_CHANNELS:\n",
    "        return \"INDIRECT (5 channels)\"\n",
    "    elif channel_indices == ALL_CHANNELS:\n",
    "        return \"ALL (8 channels)\"\n",
    "    \n",
    "    # For single channel, return the descriptive name\n",
    "    if len(channel_indices) == 1:\n",
    "        return get_channel_name(channel_indices[0])\n",
    "    \n",
    "    # For multiple channels, return a descriptive label\n",
    "    names = [get_channel_name(idx) for idx in channel_indices]\n",
    "    if len(names) <= 3:\n",
    "        return ', '.join(names)\n",
    "    else:\n",
    "        return f\"{len(names)} channels: {', '.join(names[:2])}, ...\"\n",
    "\n",
    "def get_canonical_sort_key(channels_label, channels_list=None):\n",
    "    \"\"\"Get sort key for canonical ordering based on channel_config.json.\"\"\"\n",
    "    # Groups come after single channels\n",
    "    if channels_label == \"DIRECT (3 channels)\":\n",
    "        return (1, GROUP_ORDER.index(\"DIRECT\"))\n",
    "    elif channels_label == \"INDIRECT (5 channels)\":\n",
    "        return (1, GROUP_ORDER.index(\"INDIRECT\"))\n",
    "    elif channels_label == \"ALL (8 channels)\":\n",
    "        return (1, GROUP_ORDER.index(\"ALL\"))\n",
    "    \n",
    "    # Single channels: sort by their position in SINGLE_CHANNEL_ORDER\n",
    "    if channels_list is not None and len(channels_list) == 1:\n",
    "        channel_idx = channels_list[0]\n",
    "        if channel_idx in SINGLE_CHANNEL_ORDER:\n",
    "            return (0, SINGLE_CHANNEL_ORDER.index(channel_idx))\n",
    "    \n",
    "    # Fallback: alphabetical\n",
    "    return (2, channels_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_style",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set larger font sizes for presentations\n",
    "sns.set_context('talk')\n",
    "\n",
    "# Set colorblind-friendly Okabe-Ito palette globally\n",
    "cm_mpl = Colormap('okabeito:okabeito').to_mpl()\n",
    "okabe_ito_colors = [cm_mpl(i / 7) for i in range(8)]\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(okabe_ito_colors)\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=okabe_ito_colors)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "# Set serif font LAST to ensure it's not overwritten\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parse_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_test_results(file_path):\n",
    "    \"\"\"Parse a test_results.txt file and extract key information.\"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    # Extract channels used\n",
    "    channels_match = re.search(r'Channels Used: \\[([\\d, ]+)\\]', content)\n",
    "    if channels_match:\n",
    "        channels_str = channels_match.group(1)\n",
    "        result['channels'] = [int(x.strip()) for x in channels_str.split(',')]\n",
    "        result['channels_str'] = ','.join(str(c) for c in result['channels'])\n",
    "        result['n_channels'] = len(result['channels'])\n",
    "        # Add readable channel names\n",
    "        result['channels_readable'] = [get_channel_name(c) for c in result['channels']]\n",
    "        result['channels_label'] = get_channels_label(result['channels'])\n",
    "    \n",
    "    # Extract classification target (stop at newline)\n",
    "    target_match = re.search(r'Classification Target: ([^\\n]+)', content)\n",
    "    if target_match:\n",
    "        result['classification_target'] = target_match.group(1).strip()\n",
    "    \n",
    "    # Extract metrics (handle spaces in metric names)\n",
    "    # Pattern: metric_name (with possible spaces) : value\n",
    "    metric_pattern = r'(test_[\\w\\s]+?)\\s*:\\s*([\\d.]+)'\n",
    "    for match in re.finditer(metric_pattern, content):\n",
    "        metric_name = match.group(1).strip().replace(' ', '_')\n",
    "        metric_value = float(match.group(2))\n",
    "        result[metric_name] = metric_value\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all test results from experiment_results directory\n",
    "results_dir = Path('experiment_results')\n",
    "all_results = []\n",
    "\n",
    "if not results_dir.exists():\n",
    "    raise FileNotFoundError(f\"Directory {results_dir} does not exist. Run experiments first!\")\n",
    "\n",
    "# Find all test_results.txt files\n",
    "test_result_files = list(results_dir.glob('*/test_results.txt'))\n",
    "\n",
    "print(f\"Found {len(test_result_files)} experiment result files\")\n",
    "\n",
    "for result_file in sorted(test_result_files):\n",
    "    try:\n",
    "        result = parse_test_results(result_file)\n",
    "        result['experiment_name'] = result_file.parent.name\n",
    "        all_results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {result_file}: {e}\")\n",
    "\n",
    "print(f\"Successfully parsed {len(all_results)} results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create_dataframe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by classification target and channels for better readability\n",
    "df = df.sort_values(['classification_target', 'n_channels', 'channels_str']).reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_csv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "csv_path = results_dir / 'all_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved combined results to {csv_path}\")\n",
    "print(f\"CSV contains {len(df)} experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summary_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nExperiments by Classification Target:\")\n",
    "print(df['classification_target'].value_counts())\n",
    "\n",
    "print(\"\\nExperiments by Number of Channels:\")\n",
    "print(df['n_channels'].value_counts().sort_index())\n",
    "\n",
    "# Get F1 columns\n",
    "f1_columns = [col for col in df.columns if col.startswith('test_f1_')]\n",
    "\n",
    "if f1_columns:\n",
    "    print(\"\\nF1 Score Statistics:\")\n",
    "    for col in f1_columns:\n",
    "        target = col.replace('test_f1_', '').replace('_', ' ').title()\n",
    "        print(f\"\\n  {target}:\")\n",
    "        print(f\"    Mean: {df[col].mean():.4f}\")\n",
    "        print(f\"    Std:  {df[col].std():.4f}\")\n",
    "        print(f\"    Min:  {df[col].min():.4f}\")\n",
    "        print(f\"    Max:  {df[col].max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prepare_heatmap_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for heatmap visualization\n",
    "# We'll create separate heatmaps for each classification target\n",
    "\n",
    "def create_heatmap_data(df, target_col):\n",
    "    \"\"\"Create a pivot table for heatmap visualization.\"\"\"\n",
    "    # Filter for experiments that have this target\n",
    "    target_name = target_col.replace('test_f1_', '').replace('_', ' ')\n",
    "    df_target = df[df[target_col].notna()].copy()\n",
    "    \n",
    "    if len(df_target) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Create a readable label for channel configuration\n",
    "    df_target['channel_label'] = df_target.apply(\n",
    "        lambda row: f\"{row['n_channels']}ch: [{row['channels_str']}]\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Create pivot table\n",
    "    pivot = df_target.pivot_table(\n",
    "        values=target_col,\n",
    "        index='channel_label',\n",
    "        columns='classification_target',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    return pivot, target_name\n",
    "\n",
    "# Get all F1 score columns\n",
    "f1_columns = [col for col in df.columns if col.startswith('test_f1_')]\n",
    "print(f\"Found F1 columns: {f1_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_combined_heatmap",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a combined heatmap showing all targets\n",
    "# Rows: Channel configurations, Columns: Classification targets, Values: F1 scores\n",
    "\n",
    "# Use the readable channel labels that were created during parsing\n",
    "# Note: channels_label was already created in parse_test_results using get_channels_label()\n",
    "\n",
    "# Melt the dataframe to long format for all F1 scores\n",
    "f1_cols = [col for col in df.columns if col.startswith('test_f1_')]\n",
    "df_long = df.melt(\n",
    "    id_vars=['experiment_name', 'channels_label', 'channels_str', 'channels', 'n_channels', 'classification_target'],\n",
    "    value_vars=f1_cols,\n",
    "    var_name='metric',\n",
    "    value_name='f1_score'\n",
    ")\n",
    "\n",
    "# Remove rows with NaN F1 scores\n",
    "df_long = df_long[df_long['f1_score'].notna()]\n",
    "\n",
    "# Extract the target from the metric name\n",
    "df_long['target_from_metric'] = df_long['metric'].str.replace('test_f1_', '').str.replace('_', ' ')\n",
    "\n",
    "# Create a combined label showing what was predicted\n",
    "df_long['prediction_label'] = df_long.apply(\n",
    "    lambda row: f\"{row['target_from_metric']}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create pivot table for the main heatmap\n",
    "pivot_main = df_long.pivot_table(\n",
    "    values='f1_score',\n",
    "    index='channels_label',\n",
    "    columns='prediction_label',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Sort using canonical order from channel_config.json\n",
    "# Build mapping from label to channels list for sorting (handle unhashable lists)\n",
    "label_to_channels = {}\n",
    "for _, row in df[['channels_label', 'channels']].iterrows():\n",
    "    label_to_channels[row['channels_label']] = row['channels']\n",
    "\n",
    "# Create sorting keys using canonical order\n",
    "sort_keys = [(get_canonical_sort_key(idx, label_to_channels.get(idx)), idx) for idx in pivot_main.index]\n",
    "sorted_indices = [x[1] for x in sorted(sort_keys)]\n",
    "pivot_main = pivot_main.loc[sorted_indices]\n",
    "\n",
    "# Plot the combined heatmap\n",
    "fig, ax = plt.subplots(figsize=(max(8, len(pivot_main.columns) * 1.5), max(10, len(pivot_main) * 0.5)))\n",
    "\n",
    "sns.heatmap(\n",
    "    pivot_main,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'F1 Score'},\n",
    "    ax=ax,\n",
    "    linewidths=0.5,\n",
    "    linecolor='gray',\n",
    "    annot_kws={'fontsize': 14}\n",
    ")\n",
    "\n",
    "ax.set_title('F1 Scores by Channel Configuration and Prediction Target', pad=20)\n",
    "ax.set_xlabel('Prediction Target')\n",
    "ax.set_ylabel('Channel Configuration')\n",
    "\n",
    "# Rotate labels for better readability\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'f1_scores_heatmap.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(results_dir / 'f1_scores_heatmap.png', bbox_inches='tight', dpi=300)\n",
    "print(f\"Saved heatmap to {results_dir / 'f1_scores_heatmap.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_detailed_heatmaps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate detailed heatmaps for Deep Drawing and Ironing\n",
    "# This will show: Rows = Channel configs, Columns = Classification target used\n",
    "\n",
    "for f1_col in f1_cols:\n",
    "    target_name = f1_col.replace('test_f1_', '').replace('_', ' ').title()\n",
    "    \n",
    "    # Filter for experiments with this F1 score\n",
    "    df_filtered = df[df[f1_col].notna()].copy()\n",
    "    \n",
    "    if len(df_filtered) == 0:\n",
    "        continue\n",
    "    \n",
    "    # Create pivot: Rows = channel configs, Columns = classification target\n",
    "    pivot = df_filtered.pivot_table(\n",
    "        values=f1_col,\n",
    "        index='channels_label',\n",
    "        columns='classification_target',\n",
    "        aggfunc='mean'\n",
    "    )\n",
    "    \n",
    "    # Sort using canonical order from channel_config.json\n",
    "    # Build mapping from label to channels list (handle unhashable lists)\n",
    "    label_to_channels = {}\n",
    "    for _, row in df_filtered[['channels_label', 'channels']].iterrows():\n",
    "        label_to_channels[row['channels_label']] = row['channels']\n",
    "    \n",
    "    sort_keys = [(get_canonical_sort_key(idx, label_to_channels.get(idx)), idx) for idx in pivot.index]\n",
    "    sorted_indices = [x[1] for x in sorted(sort_keys)]\n",
    "    pivot = pivot.loc[sorted_indices]\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(max(6, len(pivot.columns) * 2), max(10, len(pivot) * 0.5)))\n",
    "    \n",
    "    sns.heatmap(\n",
    "        pivot,\n",
    "        annot=True,\n",
    "        fmt='.3f',\n",
    "        cmap='RdYlGn',\n",
    "        vmin=0,\n",
    "        vmax=1,\n",
    "        cbar_kws={'label': 'F1 Score'},\n",
    "        ax=ax,\n",
    "        linewidths=0.5,\n",
    "        linecolor='gray',\n",
    "        annot_kws={'fontsize': 14}\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f'{target_name} F1 Scores\\nby Channel Configuration and Model Target', pad=20)\n",
    "    ax.set_xlabel('Classification Target (What model was trained to predict)')\n",
    "    ax.set_ylabel('Channel Configuration')\n",
    "    \n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    filename = f\"{target_name.lower().replace(' ', '_')}_f1_heatmap\"\n",
    "    plt.savefig(results_dir / f'{filename}.pdf', bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(results_dir / f'{filename}.png', bbox_inches='tight', dpi=300)\n",
    "    print(f\"Saved {target_name} heatmap to {results_dir / filename}.pdf\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "top_performers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top performing configurations for each target\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP PERFORMING CONFIGURATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for f1_col in f1_cols:\n",
    "    target_name = f1_col.replace('test_f1_', '').replace('_', ' ').title()\n",
    "    \n",
    "    df_sorted = df[df[f1_col].notna()].sort_values(f1_col, ascending=False)\n",
    "    \n",
    "    print(f\"\\n{target_name} - Top 5 Configurations:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in df_sorted.head(5).iterrows():\n",
    "        print(f\"  {row[f1_col]:.4f} - {row['channels_label']} - Target: {row['classification_target']}\")\n",
    "    \n",
    "    print(f\"\\n{target_name} - Bottom 5 Configurations:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for idx, row in df_sorted.tail(5).iterrows():\n",
    "        print(f\"  {row[f1_col]:.4f} - {row['channels_label']} - Target: {row['classification_target']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "channel_importance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze single-channel performance to understand channel importance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SINGLE CHANNEL PERFORMANCE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_single = df[df['n_channels'] == 1].copy()\n",
    "\n",
    "if len(df_single) > 0:\n",
    "    # Extract single channel number\n",
    "    df_single['channel_num'] = df_single['channels'].apply(lambda x: x[0] if len(x) == 1 else None)\n",
    "    df_single['channel_name'] = df_single['channel_num'].apply(get_channel_name)\n",
    "    \n",
    "    for f1_col in f1_cols:\n",
    "        target_name = f1_col.replace('test_f1_', '').replace('_', ' ').title()\n",
    "        \n",
    "        df_target = df_single[df_single[f1_col].notna()].copy()\n",
    "        \n",
    "        if len(df_target) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{target_name}:\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Group by channel name for better readability\n",
    "        channel_performance = df_target.groupby('channel_name')[f1_col].mean().sort_values(ascending=False)\n",
    "        \n",
    "        for channel_name, score in channel_performance.items():\n",
    "            print(f\"  {channel_name}: {score:.4f}\")\n",
    "else:\n",
    "    print(\"No single-channel experiments found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accuracy_vs_f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy vs F1 score to see if there are any anomalies\n",
    "fig, axes = plt.subplots(1, len(f1_cols), figsize=(7 * len(f1_cols), 6))\n",
    "\n",
    "if len(f1_cols) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, f1_col in zip(axes, f1_cols):\n",
    "    target_name = f1_col.replace('test_f1_', '').replace('_', ' ').title()\n",
    "    acc_col = f1_col.replace('f1', 'acc')\n",
    "    \n",
    "    if acc_col not in df.columns:\n",
    "        continue\n",
    "    \n",
    "    df_plot = df[[f1_col, acc_col, 'n_channels']].dropna()\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        df_plot[acc_col], \n",
    "        df_plot[f1_col],\n",
    "        c=df_plot['n_channels'],\n",
    "        cmap='viridis',\n",
    "        s=100,\n",
    "        alpha=0.6,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5\n",
    "    )\n",
    "    \n",
    "    ax.plot([0, 1], [0, 1], 'r--', alpha=0.3, label='Perfect correlation')\n",
    "    \n",
    "    ax.set_xlabel('Test Accuracy')\n",
    "    ax.set_ylabel('Test F1 Score')\n",
    "    ax.set_title(f'{target_name}\\nAccuracy vs F1 Score')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('Number of Channels')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(results_dir / 'accuracy_vs_f1.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig(results_dir / 'accuracy_vs_f1.png', bbox_inches='tight', dpi=300)\n",
    "print(f\"Saved accuracy vs F1 plot to {results_dir / 'accuracy_vs_f1.pdf'}\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
