{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "assert data_dir.exists(), f\"Data directory does not exist: {data_dir}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2ea6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "from lightning.pytorch import LightningDataModule, LightningModule, Trainer\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import wandb\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from cmap import Colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "torch.set_float32_matmul_precision(\"medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d441372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Times New Roman font and larger font sizes for presentations\n",
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "sns.set_context('talk')\n",
    "\n",
    "# Set colorblind-friendly Okabe-Ito palette globally\n",
    "cm_mpl = Colormap('okabeito:okabeito').to_mpl()\n",
    "okabe_ito_colors = [cm_mpl(i / 7)\n",
    "                    for i in range(8)]  # Extract 8 distinct colors\n",
    "sns.set_style('whitegrid')\n",
    "sns.set_palette(okabe_ito_colors)\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler(color=okabe_ito_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd9b57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838b838",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNEL_MAPPING = {\n",
    "    \"time_s\": \"Time (s)\",\n",
    "    \"K1_Ch1_Mod2__AI0\": \"Strip Connection Cut (Direct)\",\n",
    "    \"K1_Ch2_Mod2__AI1\": \"Deep Drawing (Direct)\",\n",
    "    \"K1_Ch3_Mod2__AI2\": \"Ironing (Direct)\",\n",
    "    \"K2_Ch1_Mod2__AI3\": \"Deep Drawing - Top (Indirect)\",\n",
    "    \"K2_Ch2_Mod2__AI4\": \"Deep Drawing - Bottom (Indirect)\",\n",
    "    \"K2_Ch3_Mod2__AI5\": \"Not Connected\",\n",
    "    \"K3_Ch1_Mod2__AI6\": \"Ironing - Top (Indirect)\",\n",
    "    \"K3_Ch2_Mod2__AI7\": \"Ironing - Stemp Holder (Indirect)\",\n",
    "    \"K3_Ch3_Mod2__AI8\": \"Ironing - Bottom (Indirect)\",\n",
    "    \"Mod2__AI9\": \"Mod2/AI9\",  # Unclear channel\n",
    "    \"Mod7_A0\": \"Thermocouple 1\",\n",
    "    \"Mod7_A1\": \"Thermocouple 2\",\n",
    "    \"Mod7_A2\": \"Mod7 A2\",  # Unclear channel\n",
    "    \"Mod7_A3\": \"Mod7 A3\",  # Unclear channel\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class SignalPortion(Enum):\n",
    "    Single = \"Single\"\n",
    "    DIRECT = [1, 2, 3]\n",
    "    INDIRECT = [4, 5, 7, 8, 9]\n",
    "    ALL = DIRECT + INDIRECT\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class ClassificationTarget(Enum):\n",
    "    DeepDrawing = \"Deep Drawing\"\n",
    "    Ironing = \"Ironing\"\n",
    "    Both = \"Deep Drawing and Ironing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parameters_cell",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters cell for papermill\n",
    "# These values will be overridden by papermill when running experiments\n",
    "data_path = \"all_data.hdf5\"\n",
    "signal_only_channels = [1]\n",
    "classification_target = \"Ironing\"\n",
    "plot_path = None  # Will be set below if None\n",
    "wandb_group = None  # Optional WandB group name for organizing experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - uses parameters from papermill or defaults from parameters cell\n",
    "from pathlib import Path\n",
    "import ast\n",
    "\n",
    "# Convert signal_only_channels if it's a string (from shell parameter)\n",
    "if isinstance(signal_only_channels, str):\n",
    "    signal_only_channels = ast.literal_eval(signal_only_channels)\n",
    "\n",
    "# Convert data_path to Path object\n",
    "if not isinstance(data_path, Path):\n",
    "    if Path(data_path).is_absolute():\n",
    "        data_path = Path(data_path)\n",
    "    else:\n",
    "        data_path = data_dir / data_path\n",
    "\n",
    "# Get readable channel names\n",
    "signal_only_channels_readable = [\n",
    "    CHANNEL_MAPPING[list(CHANNEL_MAPPING.keys())[i]] for i in signal_only_channels]\n",
    "\n",
    "# Convert classification_target string to enum\n",
    "if isinstance(classification_target, str):\n",
    "    classification_target = ClassificationTarget[classification_target]\n",
    "\n",
    "# Set plot_path if not provided\n",
    "if plot_path is None:\n",
    "    plot_path = data_path.parents[1] / \"plots\" / \\\n",
    "        f\"{classification_target.value}-from-{','.join(signal_only_channels_readable)}\"\n",
    "elif not isinstance(plot_path, Path):\n",
    "    plot_path = Path(plot_path)\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  data_path: {data_path}\")\n",
    "print(f\"  signal_only_channels: {signal_only_channels}\")\n",
    "print(f\"  classification_target: {classification_target.value}\")\n",
    "print(f\"  plot_path: {plot_path.absolute()}\")\n",
    "\n",
    "# Create plot directory if it doesn't exist\n",
    "plot_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c21bfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SheetMetalDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset for sheet metal forming measurements.\"\"\"\n",
    "\n",
    "    def __init__(self, data: np.ndarray, metadata: dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data: Array of shape (n_entries, n_signals, n_points)\n",
    "            metadata: Dict with Deep Drawing, Ironing arrays\n",
    "        \"\"\"\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.deep_drawing = torch.from_numpy(metadata[\"Deep Drawing\"]).long()-1\n",
    "        self.ironing = torch.from_numpy(metadata[\"Ironing\"]).long()-1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"data\": self.data[idx],\n",
    "            \"Deep Drawing\": self.deep_drawing[idx],\n",
    "            \"Ironing\": self.ironing[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "class SheetMetalDataModule(LightningDataModule):\n",
    "    \"\"\"LightningDataModule for sheet metal forming data.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: Path,\n",
    "        signal_only_channels: list = None,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        train_ratio: float = 0.7,\n",
    "        val_ratio: float = 0.1,\n",
    "        seed: int = 42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        self.signal_only_channels = signal_only_channels if signal_only_channels is not None else []\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.train_ratio = train_ratio\n",
    "        self.val_ratio = val_ratio\n",
    "        self.seed = seed\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"data_path\"])\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "        # Load data from HDF5\n",
    "        with h5py.File(self.data_path, \"r\") as h5f:\n",
    "            data = h5f[\"data\"][:, self.signal_only_channels, :]\n",
    "            all_signal_keys = [k for k in h5f[\"data\"].attrs[\"signal_keys\"]]\n",
    "            self.signal_keys = [all_signal_keys[i]\n",
    "                                for i in self.signal_only_channels]\n",
    "            metadata = {\n",
    "                \"Deep Drawing\": h5f[\"meta_data/Deep Drawing\"][:],\n",
    "                \"Ironing\": h5f[\"meta_data/Ironing\"][:],\n",
    "            }\n",
    "\n",
    "        # Data is already filtered in prepare-data.ipynb (T=0 and A=0 removed)\n",
    "        print(f\"Loaded {len(data)} samples from {self.data_path}\")\n",
    "\n",
    "        # Reproducible shuffle and split\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        indices = rng.permutation(len(data))\n",
    "\n",
    "        n_train = int(len(data) * self.train_ratio)\n",
    "        n_val = int(len(data) * self.val_ratio)\n",
    "\n",
    "        def make_dataset(idx):\n",
    "            return SheetMetalDataset(data[idx], {k: v[idx] for k, v in metadata.items()})\n",
    "\n",
    "        self.train_dataset = make_dataset(indices[:n_train])\n",
    "        self.val_dataset = make_dataset(indices[n_train:n_train + n_val])\n",
    "        self.test_dataset = make_dataset(indices[n_train + n_val:])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "\n",
    "data_module = SheetMetalDataModule(\n",
    "    data_path=data_path,\n",
    "    signal_only_channels=signal_only_channels,\n",
    ")\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the data module\n",
    "print(f\"Signal keys: {data_module.signal_keys}\")\n",
    "print(f\"Train samples: {len(data_module.train_dataset)}\")\n",
    "print(f\"Val samples: {len(data_module.val_dataset)}\")\n",
    "print(f\"Test samples: {len(data_module.test_dataset)}\")\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(data_module.train_dataloader()))\n",
    "print(f\"\\nBatch data shape: {batch['data'].shape}\")\n",
    "print(f\"Batch Deep Drawing: {batch['Deep Drawing']}\")\n",
    "print(f\"Batch Ironing: {batch['Ironing']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12123212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show one instance\n",
    "sample = data_module.train_dataset[0]\n",
    "print(\"Sample data shape:\", sample[\"data\"].shape)\n",
    "print(\"Sample Deep Drawing:\", sample[\"Deep Drawing\"].item())\n",
    "print(\"Sample Ironing:\", sample[\"Ironing\"].item())\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, signal in enumerate(data_module.signal_keys):\n",
    "    plt.plot(sample[\"data\"][i].numpy(), label=signal)\n",
    "plt.title(\"Sheet Metal Forming Signals\")\n",
    "plt.xlabel(\"Time Points\")\n",
    "plt.ylabel(\"Signal Values\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"sample_signals.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'sample_signals.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e896e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class label frequencies for Deep Drawing and Ironing\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "for ax, label in zip(axes, [\"Deep Drawing\", \"Ironing\"]):\n",
    "    all_labels = torch.cat([\n",
    "        getattr(data_module.train_dataset, label.lower().replace(\" \", \"_\")),\n",
    "        getattr(data_module.val_dataset, label.lower().replace(\" \", \"_\")),\n",
    "        getattr(data_module.test_dataset, label.lower().replace(\" \", \"_\")),\n",
    "    ])\n",
    "    unique, counts = torch.unique(all_labels, return_counts=True)\n",
    "    freq = counts.float()\n",
    "    sns.barplot(x=unique.numpy(), y=freq.numpy(), ax=ax)\n",
    "    ax.set_title(f\"Label Distribution ({label})\")\n",
    "    ax.set_xlabel(\"Class Label\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"label_distribution.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'label_distribution.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001f574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple 1d-CNN to predict Deep Drawing and Ironing from signals\n",
    "\n",
    "def conv_block(in_ch, out_ch, kernel_size, pool_size, dropout):\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Conv1d(in_ch, out_ch, kernel_size, padding=kernel_size // 2),\n",
    "        torch.nn.BatchNorm1d(out_ch),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.MaxPool1d(pool_size),\n",
    "        torch.nn.Dropout(dropout),\n",
    "    )\n",
    "\n",
    "\n",
    "class SimpleCNN(LightningModule):\n",
    "    def __init__(self, n_signals: int, classification_target: ClassificationTarget, lr: float = 5e-4, dropout: float = 0.25):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.lr = lr\n",
    "        self.classification_target = classification_target\n",
    "        n_classes = {\n",
    "            \"Deep Drawing\": data_module.train_dataset.deep_drawing.unique().numel(),\n",
    "            \"Ironing\": data_module.train_dataset.ironing.unique().numel(),\n",
    "        }\n",
    "\n",
    "        # 1100 -> 550 -> 137 -> 34 -> 8 -> 1 (AdaptiveAvgPool handles variable input)\n",
    "        self.backbone = torch.nn.Sequential(\n",
    "            conv_block(n_signals, 32, kernel_size=7,\n",
    "                       pool_size=2, dropout=dropout * 0.5),\n",
    "            conv_block(32, 64, kernel_size=5, pool_size=4,\n",
    "                       dropout=dropout * 0.5),\n",
    "            conv_block(64, 128, kernel_size=5, pool_size=4, dropout=dropout),\n",
    "            conv_block(128, 256, kernel_size=3, pool_size=4, dropout=dropout),\n",
    "            torch.nn.AdaptiveAvgPool1d(1),\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "        # Create heads based on classification target\n",
    "        targets_to_track = []\n",
    "        if classification_target in [ClassificationTarget.DeepDrawing, ClassificationTarget.Both]:\n",
    "            self.head_deep_drawing = torch.nn.Linear(\n",
    "                256, n_classes[\"Deep Drawing\"])\n",
    "            targets_to_track.append(\"Deep Drawing\")\n",
    "        else:\n",
    "            self.head_deep_drawing = None\n",
    "\n",
    "        if classification_target in [ClassificationTarget.Ironing, ClassificationTarget.Both]:\n",
    "            self.head_ironing = torch.nn.Linear(256, n_classes[\"Ironing\"])\n",
    "            targets_to_track.append(\"Ironing\")\n",
    "        else:\n",
    "            self.head_ironing = None\n",
    "\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        # Metrics for each stage and label\n",
    "        for stage in [\"train\", \"val\", \"test\"]:\n",
    "            for label in targets_to_track:\n",
    "                num_classes = n_classes[label]\n",
    "                setattr(self, f\"{stage}_acc_{label.lower().replace(' ', '_')}\",\n",
    "                        torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes))\n",
    "                setattr(self, f\"{stage}_f1_{label.lower().replace(' ', '_')}\",\n",
    "                        torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\"))\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        # Return outputs based on classification target\n",
    "        if self.classification_target == ClassificationTarget.Both:\n",
    "            return self.head_deep_drawing(features), self.head_ironing(features)\n",
    "        elif self.classification_target == ClassificationTarget.DeepDrawing:\n",
    "            return self.head_deep_drawing(features), None\n",
    "        else:  # ClassificationTarget.Ironing\n",
    "            return None, self.head_ironing(features)\n",
    "\n",
    "    def _step(self, batch, stage: str):\n",
    "        data, deep_drawing, ironing = batch[\"data\"], batch[\"Deep Drawing\"], batch[\"Ironing\"]\n",
    "        deep_drawing_out, ironing_out = self(data)\n",
    "\n",
    "        # Compute loss only for active targets\n",
    "        loss = 0.0\n",
    "        if self.classification_target in [ClassificationTarget.DeepDrawing, ClassificationTarget.Both]:\n",
    "            loss += self.criterion(deep_drawing_out, deep_drawing)\n",
    "        if self.classification_target in [ClassificationTarget.Ironing, ClassificationTarget.Both]:\n",
    "            loss += self.criterion(ironing_out, ironing)\n",
    "\n",
    "        # Update and log metrics only for active targets\n",
    "        if self.classification_target in [ClassificationTarget.DeepDrawing, ClassificationTarget.Both]:\n",
    "            acc = getattr(self, f\"{stage}_acc_deep_drawing\")\n",
    "            f1 = getattr(self, f\"{stage}_f1_deep_drawing\")\n",
    "            acc(deep_drawing_out, deep_drawing)\n",
    "            f1(deep_drawing_out, deep_drawing)\n",
    "            self.log(f\"{stage}_acc_Deep Drawing\",\n",
    "                     acc, prog_bar=(stage != \"train\"))\n",
    "            self.log(f\"{stage}_f1_Deep Drawing\", f1,\n",
    "                     prog_bar=(stage != \"train\"))\n",
    "\n",
    "        if self.classification_target in [ClassificationTarget.Ironing, ClassificationTarget.Both]:\n",
    "            acc = getattr(self, f\"{stage}_acc_ironing\")\n",
    "            f1 = getattr(self, f\"{stage}_f1_ironing\")\n",
    "            acc(ironing_out, ironing)\n",
    "            f1(ironing_out, ironing)\n",
    "            self.log(f\"{stage}_acc_Ironing\", acc, prog_bar=(stage != \"train\"))\n",
    "            self.log(f\"{stage}_f1_Ironing\", f1, prog_bar=(stage != \"train\"))\n",
    "\n",
    "        self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._step(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "\n",
    "# Check label ranges\n",
    "train_deep_drawing = data_module.train_dataset.deep_drawing\n",
    "train_ironing = data_module.train_dataset.ironing\n",
    "print(f\"Deep Drawing: {train_deep_drawing.min().item()}-{train_deep_drawing.max().item()}, Ironing: {train_ironing.min().item()}-{train_ironing.max().item()}\")\n",
    "\n",
    "model = SimpleCNN(\n",
    "    n_signals=len(data_module.signal_keys),\n",
    "    classification_target=classification_target,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669a3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data shapes through the model\n",
    "from torchinfo import summary\n",
    "\n",
    "# Get input shape from data\n",
    "sample_batch = next(iter(data_module.train_dataloader()))\n",
    "input_shape = sample_batch[\"data\"].shape  # (batch, n_signals, n_points)\n",
    "\n",
    "summary(model, input_size=input_shape, col_names=[\n",
    "        \"input_size\", \"output_size\", \"num_params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef317120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WandB logger for learning curve tracking\n",
    "# Create tags for filtering experiments: signal channels + classification target\n",
    "wandb_tags = [f\"signal_{ch}\" for ch in signal_only_channels]\n",
    "# e.g., \"deepdrawing\", \"ironing\", \"both\"\n",
    "wandb_tags.append(classification_target.name.lower())\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"sheet-metal-forming\",\n",
    "    name=\"SimpleCNN\",\n",
    "    group=wandb_group,  # Optional group name for organizing experiments\n",
    "    tags=wandb_tags,\n",
    "    log_model=False,\n",
    "    save_dir=\"wandb\",\n",
    ")\n",
    "\n",
    "# Log hyperparameters\n",
    "wandb_logger.experiment.config.update({\n",
    "    \"signal_only_channels\": signal_only_channels,\n",
    "    \"classification_target\": classification_target.value,\n",
    "    \"n_signals\": len(data_module.signal_keys),\n",
    "    \"batch_size\": data_module.batch_size,\n",
    "    \"train_samples\": len(data_module.train_dataset),\n",
    "    \"val_samples\": len(data_module.val_dataset),\n",
    "    \"test_samples\": len(data_module.test_dataset),\n",
    "})\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=20,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    precision=\"bf16-mixed\",\n",
    "    # fast_dev_run=True,  # Set to False to run full training\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor=\"val_loss\", mode=\"min\",\n",
    "                      patience=5, verbose=True),\n",
    "        ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1),\n",
    "    ],\n",
    "    logger=wandb_logger,\n",
    ")\n",
    "trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "print()  # Blank line for readability\n",
    "print(\"Load the best model after training\")\n",
    "model = SimpleCNN.load_from_checkpoint(\n",
    "    trainer.checkpoint_callback.best_model_path, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = trainer.test(model, datamodule=data_module)\n",
    "print(\"Test results:\", test_results)\n",
    "\n",
    "# Save test results to text file\n",
    "results_file = plot_path / \"test_results.txt\"\n",
    "with open(results_file, \"w\") as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"Test Results for Sheet Metal Forming Model\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Configuration:\\n\")\n",
    "    f.write(f\"  Channels Used: {signal_only_channels}\\n\")\n",
    "    f.write(f\"  Classification Target: {classification_target.value}\\n\")\n",
    "    f.write(f\"  Number of Signals: {len(data_module.signal_keys)}\\n\")\n",
    "    f.write(f\"  Signal Keys: {data_module.signal_keys}\\n\\n\")\n",
    "\n",
    "    f.write(\"Dataset Sizes:\\n\")\n",
    "    f.write(f\"  Train: {len(data_module.train_dataset)}\\n\")\n",
    "    f.write(f\"  Validation: {len(data_module.val_dataset)}\\n\")\n",
    "    f.write(f\"  Test: {len(data_module.test_dataset)}\\n\\n\")\n",
    "\n",
    "    f.write(\"Test Performance:\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for result_dict in test_results:\n",
    "        for metric_name, value in sorted(result_dict.items()):\n",
    "            f.write(f\"  {metric_name:30s}: {value:.6f}\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"\\nSaved test results to {results_file}\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5372c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution analysis using Captum Integrated Gradients\n",
    "from captum.attr import IntegratedGradients\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Put model in eval mode and move to GPU if available\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Computing attributions on: {device}\")\n",
    "\n",
    "# Create wrapper functions for each head (Captum needs single output)\n",
    "# and IG instances based on classification target\n",
    "ig_instances = {}\n",
    "all_attributions = {}  # Store absolute values for aggregate plots\n",
    "all_attributions_signed = {}  # Store signed values for colored time series\n",
    "\n",
    "if classification_target in [ClassificationTarget.DeepDrawing, ClassificationTarget.Both]:\n",
    "    def forward_deep_drawing(x):\n",
    "        return model(x)[0]\n",
    "    ig_instances[\"Deep Drawing\"] = IntegratedGradients(forward_deep_drawing)\n",
    "    all_attributions[\"Deep Drawing\"] = []\n",
    "    all_attributions_signed[\"Deep Drawing\"] = []\n",
    "\n",
    "if classification_target in [ClassificationTarget.Ironing, ClassificationTarget.Both]:\n",
    "    def forward_ironing(x):\n",
    "        return model(x)[1]\n",
    "    ig_instances[\"Ironing\"] = IntegratedGradients(forward_ironing)\n",
    "    all_attributions[\"Ironing\"] = []\n",
    "    all_attributions_signed[\"Ironing\"] = []\n",
    "\n",
    "# Process all test samples in batches\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "print(\n",
    "    f\"Computing attributions over {len(data_module.test_dataset)} test samples for {classification_target.value}...\")\n",
    "for test_batch in tqdm(test_loader, desc=\"Computing attributions\"):\n",
    "    inputs = test_batch[\"data\"].to(device)\n",
    "    targets = {\n",
    "        \"Deep Drawing\": test_batch[\"Deep Drawing\"].to(device),\n",
    "        \"Ironing\": test_batch[\"Ironing\"].to(device),\n",
    "    }\n",
    "\n",
    "    # Baseline is zeros (no signal)\n",
    "    baseline = torch.zeros_like(inputs)\n",
    "\n",
    "    # Compute attributions for active targets\n",
    "    for target_name, ig in ig_instances.items():\n",
    "        attr = ig.attribute(inputs, baselines=baseline,\n",
    "                            target=targets[target_name])\n",
    "        # Store both signed and absolute versions (move to CPU to save GPU memory)\n",
    "        all_attributions_signed[target_name].append(attr.cpu())\n",
    "        all_attributions[target_name].append(attr.abs().cpu())\n",
    "\n",
    "# Concatenate all attributions and normalize\n",
    "normalized_attributions = {}\n",
    "percentiles = {}\n",
    "EPS = 1e-9\n",
    "\n",
    "for target_name in all_attributions.keys():\n",
    "    # Process absolute attributions for aggregate plots\n",
    "    all_attr = torch.cat(all_attributions[target_name], dim=0)\n",
    "\n",
    "    # Normalize each sample across all dimensions (n_signals x n_timepoints)\n",
    "    # so that each sample's total attribution sums to 1, preserving probabilistic interpretation\n",
    "    all_attr_normalized = all_attr / \\\n",
    "        (all_attr.sum(dim=(1, 2), keepdim=True) + EPS)\n",
    "\n",
    "    # Average normalized attributions across samples\n",
    "    avg_attr = all_attr_normalized.mean(dim=0).detach().numpy()\n",
    "\n",
    "    # Compute percentiles for distribution-free uncertainty bands\n",
    "    q10_attr = np.percentile(all_attr_normalized.numpy(), 10, axis=0)\n",
    "    q90_attr = np.percentile(all_attr_normalized.numpy(), 90, axis=0)\n",
    "\n",
    "    # Also store signed attributions (not normalized by magnitude, just raw)\n",
    "    all_attr_signed = torch.cat(all_attributions_signed[target_name], dim=0)\n",
    "\n",
    "    normalized_attributions[target_name] = {\n",
    "        \"all\": all_attr_normalized,  # Absolute, normalized\n",
    "        \"all_signed\": all_attr_signed,  # Signed, not normalized\n",
    "        \"mean\": avg_attr,\n",
    "        \"q10\": q10_attr,\n",
    "        \"q90\": q90_attr,\n",
    "    }\n",
    "\n",
    "n_samples = len(data_module.test_dataset)\n",
    "print(\n",
    "    f\"Attribution shape: {list(all_attributions.values())[0][0].shape if all_attributions else 'N/A'}\")\n",
    "print(\n",
    "    f\"Computed attributions for targets: {list(normalized_attributions.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ff08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attributions with 10-90 percentile bands\n",
    "# Normalization: each sample normalized across all (n_signals, n_timepoints),\n",
    "# then averaged across samples with percentile-based uncertainty bands (distribution-free)\n",
    "\n",
    "n_targets = len(normalized_attributions)\n",
    "fig, axes = plt.subplots(n_targets, 1, figsize=(\n",
    "    14, 4 * n_targets), sharex=True)\n",
    "\n",
    "# Handle case where there's only one target\n",
    "if n_targets == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (target_name, attr_data) in zip(axes, normalized_attributions.items()):\n",
    "    attr = attr_data[\"mean\"]\n",
    "    q10 = attr_data[\"q10\"]\n",
    "    q90 = attr_data[\"q90\"]\n",
    "\n",
    "    # attr already normalized at sample level, then averaged\n",
    "    # q10/q90 show where 80% of samples fall (10th-90th percentile - robust to non-normal distributions)\n",
    "    for i, signal_name in enumerate(data_module.signal_keys):\n",
    "        readable_name = CHANNEL_MAPPING[signal_name]\n",
    "\n",
    "        # Plot mean\n",
    "        ax.plot(attr[i], label=readable_name, alpha=0.8, linewidth=1.5)\n",
    "\n",
    "        # Add 10-90 percentile band\n",
    "        ax.fill_between(range(len(attr[i])), q10[i], q90[i], alpha=0.15)\n",
    "\n",
    "    ax.set_ylabel(f\"Relative Attribution ({target_name})\")\n",
    "    ax.set_title(\n",
    "        f\"Integrated Gradients Attribution for predicting {target_name}\\n(normalized per-sample, then averaged; shaded areas = 10th-90th percentile)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "axes[-1].set_xlabel(\"Time Steps\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"attribution_time_domain.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'attribution_time_domain.pdf'}\")\n",
    "plt.show()\n",
    "\n",
    "# Aggregated relative importance per signal (box plots)\n",
    "# Per-sample aggregation: sum over time, then normalize per sample\n",
    "n_targets = len(normalized_attributions)\n",
    "fig, axes = plt.subplots(1, n_targets, figsize=(6 * n_targets, 4))\n",
    "\n",
    "# Handle case where there's only one target\n",
    "if n_targets == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (target_name, attr_data) in zip(axes, normalized_attributions.items()):\n",
    "    all_attr_normalized = attr_data[\"all\"]\n",
    "\n",
    "    # Sum normalized attribution over time per sample\n",
    "    # (already normalized at sample level, so this shows signal-level relative importance)\n",
    "    per_sample_importance = all_attr_normalized.sum(dim=2)\n",
    "\n",
    "    labels = [CHANNEL_MAPPING[key] for key in data_module.signal_keys]\n",
    "    # Move to NumPy for seaborn\n",
    "    data_np = per_sample_importance.cpu().numpy()\n",
    "\n",
    "    sns.boxplot(data=[data_np[:, i]\n",
    "                for i in range(data_np.shape[1])], ax=ax, whis=1.5)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.set_ylabel(\"Relative Importance\")\n",
    "    ax.set_title(\n",
    "        f\"Signal Importance for {target_name}\\n(per-sample normalized, aggregated over time)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"attribution_signal_importance.pdf\",\n",
    "            bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'attribution_signal_importance.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872dc2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency-domain view of attributions (FFT)\n",
    "# Using per-sample normalized attributions then averaged\n",
    "from numpy.fft import rfft, rfftfreq\n",
    "\n",
    "# Compute FFT for each target\n",
    "fft_attributions = {}\n",
    "\n",
    "for target_name, attr_data in normalized_attributions.items():\n",
    "    # Use the first signal's length for frequency calculation\n",
    "    freqs = rfftfreq(attr_data[\"mean\"].shape[1], d=1.0)\n",
    "\n",
    "    # FFT of mean attribution\n",
    "    fft_attr = np.abs(rfft(attr_data[\"mean\"], axis=1))\n",
    "\n",
    "    # FFT of all samples for percentiles\n",
    "    fft_attr_all = np.abs(rfft(attr_data[\"all\"].numpy(), axis=2))\n",
    "    q10_fft = np.percentile(fft_attr_all, 10, axis=0)\n",
    "    q90_fft = np.percentile(fft_attr_all, 90, axis=0)\n",
    "\n",
    "    fft_attributions[target_name] = {\n",
    "        \"freqs\": freqs,\n",
    "        \"mean\": fft_attr,\n",
    "        \"q10\": q10_fft,\n",
    "        \"q90\": q90_fft,\n",
    "    }\n",
    "\n",
    "n_targets = len(fft_attributions)\n",
    "fig, axes = plt.subplots(n_targets, 1, figsize=(\n",
    "    14, 4 * n_targets), sharex=True)\n",
    "\n",
    "# Handle case where there's only one target\n",
    "if n_targets == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (target_name, fft_data) in zip(axes, fft_attributions.items()):\n",
    "    freqs = fft_data[\"freqs\"]\n",
    "    fft_attr = fft_data[\"mean\"]\n",
    "    q10_fft = fft_data[\"q10\"]\n",
    "    q90_fft = fft_data[\"q90\"]\n",
    "\n",
    "    for i, signal_name in enumerate(data_module.signal_keys):\n",
    "        readable_name = CHANNEL_MAPPING[signal_name]\n",
    "\n",
    "        # Plot mean\n",
    "        ax.plot(freqs, fft_attr[i], label=readable_name,\n",
    "                alpha=0.8, linewidth=1.5)\n",
    "\n",
    "        # Add 10-90 percentile band\n",
    "        ax.fill_between(freqs, q10_fft[i], q90_fft[i], alpha=0.15)\n",
    "\n",
    "    ax.set_ylabel(f\"FFT |Attr| ({target_name})\")\n",
    "    ax.set_title(\n",
    "        f\"Frequency-domain Attribution for predicting {target_name}\\n(normalized per-sample, then averaged; shaded areas = 10th-90th percentile)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "axes[-1].set_xlabel(\"Normalized Frequency (cycles/sample)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"attribution_fft_full.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'attribution_fft_full.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1d1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zoomed-in view of low-frequency FFT attribution (first 5%)\n",
    "freq_cutoff_pct = 0.05\n",
    "\n",
    "n_targets = len(fft_attributions)\n",
    "fig, axes = plt.subplots(n_targets, 1, figsize=(\n",
    "    14, 4 * n_targets), sharex=True)\n",
    "\n",
    "# Handle case where there's only one target\n",
    "if n_targets == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (target_name, fft_data) in zip(axes, fft_attributions.items()):\n",
    "    freqs = fft_data[\"freqs\"]\n",
    "    fft_attr = fft_data[\"mean\"]\n",
    "    q10_fft = fft_data[\"q10\"]\n",
    "    q90_fft = fft_data[\"q90\"]\n",
    "\n",
    "    freq_cutoff_idx = int(len(freqs) * freq_cutoff_pct)\n",
    "\n",
    "    for i, signal_name in enumerate(data_module.signal_keys):\n",
    "        readable_name = CHANNEL_MAPPING[signal_name]\n",
    "\n",
    "        # Plot mean in low-frequency range\n",
    "        ax.plot(freqs[:freq_cutoff_idx], fft_attr[i, :freq_cutoff_idx],\n",
    "                label=readable_name, alpha=0.8, linewidth=1.5)\n",
    "\n",
    "        # Add 10-90 percentile band\n",
    "        ax.fill_between(freqs[:freq_cutoff_idx], q10_fft[i, :freq_cutoff_idx],\n",
    "                        q90_fft[i, :freq_cutoff_idx], alpha=0.15)\n",
    "\n",
    "    ax.set_ylabel(f\"FFT |Attr| ({target_name})\")\n",
    "    ax.set_title(\n",
    "        f\"Low-Frequency Attribution for predicting {target_name} (0-{int(freq_cutoff_pct*100)}% freq range)\\n(normalized per-sample, then averaged; shaded areas = 10th-90th percentile)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "axes[-1].set_xlabel(\"Normalized Frequency (cycles/sample)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(plot_path / \"attribution_fft_lowfreq.pdf\", bbox_inches=\"tight\")\n",
    "print(f\"Saved plot to {plot_path / 'attribution_fft_lowfreq.pdf'}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "colored_attribution_plots",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attribution Visualization: Time Series with Attribution-Colored Points\n",
    "# Inspired by attr/plot_mean_attributions.py\n",
    "# Shows the actual signal values with attribution intensity as color\n",
    "\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Get a sample from each class for visualization\n",
    "# We'll show one example per target (Deep Drawing level or Ironing level)\n",
    "sample_indices_per_class = {}\n",
    "\n",
    "for target_name in normalized_attributions.keys():\n",
    "    # Get 3 samples per class (levels 0, 1, 2)\n",
    "    sample_indices_per_class[target_name] = {}\n",
    "\n",
    "    test_loader_single = torch.utils.data.DataLoader(\n",
    "        data_module.test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # Find one sample from each class\n",
    "    class_samples_found = {0: None, 1: None, 2: None}\n",
    "\n",
    "    for idx, batch in enumerate(test_loader_single):\n",
    "        if target_name == \"Deep Drawing\":\n",
    "            class_label = batch[\"Deep Drawing\"].item()\n",
    "        else:  # Ironing\n",
    "            class_label = batch[\"Ironing\"].item()\n",
    "\n",
    "        if class_label in class_samples_found and class_samples_found[class_label] is None:\n",
    "            class_samples_found[class_label] = idx\n",
    "\n",
    "        # Stop if we found all 3 classes\n",
    "        if all(v is not None for v in class_samples_found.values()):\n",
    "            break\n",
    "\n",
    "    sample_indices_per_class[target_name] = class_samples_found\n",
    "\n",
    "print(f\"Found sample indices for each class:\")\n",
    "for target_name, indices in sample_indices_per_class.items():\n",
    "    print(f\"  {target_name}: {indices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_colored_attributions",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create colored time series plots for each target\n",
    "# Using LineCollection to color the line segments based on attribution\n",
    "# Uses SIGNED attributions (not absolute) to show positive vs negative contributions\n",
    "from matplotlib.collections import LineCollection\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "\n",
    "def make_segments(x, y):\n",
    "    \"\"\"\n",
    "    Create list of line segments from x and y coordinates.\n",
    "    Returns array of shape: (n_segments, 2 points, 2 coordinates)\n",
    "    \"\"\"\n",
    "    points = np.array([x, y]).T.reshape(-1, 1, 2)\n",
    "    segments = np.concatenate([points[:-1], points[1:]], axis=1)\n",
    "    return segments\n",
    "\n",
    "\n",
    "for target_name in normalized_attributions.keys():\n",
    "    attr_data = normalized_attributions[target_name]\n",
    "\n",
    "    # Get SIGNED attribution data (not absolute!)\n",
    "    # Shape: (n_test_samples, n_signals, n_timepoints)\n",
    "    all_attr_signed = attr_data['all_signed'].numpy()\n",
    "\n",
    "    # Create figure with subplots for each class level\n",
    "    n_classes = 3\n",
    "    fig, axes = plt.subplots(n_classes, 1, figsize=(\n",
    "        14, 3.5 * n_classes), constrained_layout=True)\n",
    "\n",
    "    if n_classes == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    class_labels = ['Level 1', 'Level 2', 'Level 3']\n",
    "\n",
    "    for class_idx, ax in enumerate(axes):\n",
    "        # Get the sample index for this class\n",
    "        sample_idx = sample_indices_per_class[target_name][class_idx]\n",
    "\n",
    "        if sample_idx is None:\n",
    "            ax.text(0.5, 0.5, f'No sample found for {class_labels[class_idx]}',\n",
    "                    ha='center', va='center', transform=ax.transAxes)\n",
    "            continue\n",
    "\n",
    "        # Get the actual signal data for this sample\n",
    "        sample_data = data_module.test_dataset[sample_idx]['data'].numpy()\n",
    "\n",
    "        # Get the SIGNED attribution for this sample\n",
    "        # Shape: (n_signals, n_timepoints)\n",
    "        sample_attr_signed = all_attr_signed[sample_idx]\n",
    "\n",
    "        # Find global min/max for consistent y-axis across all signals\n",
    "        y_min = sample_data.min()\n",
    "        y_max = sample_data.max()\n",
    "\n",
    "        # Plot each signal with colored line segments\n",
    "        for signal_idx, signal_name in enumerate(data_module.signal_keys):\n",
    "            readable_name = CHANNEL_MAPPING[signal_name]\n",
    "\n",
    "            signal_values = sample_data[signal_idx]\n",
    "            signal_attr_signed = sample_attr_signed[signal_idx]\n",
    "\n",
    "            # Original time points\n",
    "            time_points_orig = np.arange(len(signal_values))\n",
    "\n",
    "            # Interpolate to create smoother line segments (5x more points)\n",
    "            interp_factor = 5\n",
    "            time_points_interp = np.linspace(\n",
    "                0, len(signal_values) - 1, len(signal_values) * interp_factor)\n",
    "\n",
    "            # Interpolate signal values\n",
    "            f_signal = interp1d(time_points_orig, signal_values, kind='cubic')\n",
    "            signal_values_interp = f_signal(time_points_interp)\n",
    "\n",
    "            # Interpolate SIGNED attributions\n",
    "            f_attr = interp1d(time_points_orig,\n",
    "                              signal_attr_signed, kind='cubic')\n",
    "            signal_attr_signed_interp = f_attr(time_points_interp)\n",
    "\n",
    "            # Normalize signed attributions symmetrically around zero for diverging colormap\n",
    "            # Find the maximum absolute value across all signals for this sample for consistent scaling\n",
    "            attr_max_abs = np.abs(sample_attr_signed).max() + 1e-9\n",
    "            attr_normalized = signal_attr_signed_interp / attr_max_abs\n",
    "            # Clip to [-1, 1] range for colormap\n",
    "            attr_normalized = np.clip(attr_normalized, -1.0, 1.0)\n",
    "\n",
    "            # Create line segments from interpolated data\n",
    "            segments = make_segments(time_points_interp, signal_values_interp)\n",
    "\n",
    "            # Create LineCollection with attribution-based colors using 'icefire' diverging colormap\n",
    "            # icefire: blue (negative) -> white/yellow (zero) -> orange/red (positive)\n",
    "            lc = LineCollection(\n",
    "                segments,\n",
    "                array=attr_normalized[:-1],  # One color per segment\n",
    "                cmap='icefire',\n",
    "                norm=plt.Normalize(-1.0, 1.0),  # Symmetric around zero\n",
    "                linewidth=2.5,\n",
    "                alpha=0.9\n",
    "            )\n",
    "\n",
    "            ax.add_collection(lc)\n",
    "\n",
    "        # Set axis limits\n",
    "        ax.set_xlim(0, len(time_points_orig) - 1)\n",
    "        ax.set_ylim(y_min * 1.05, y_max * 1.05)\n",
    "\n",
    "        ax.set_title(\n",
    "            f'{target_name} - {class_labels[class_idx]} (Test Sample {sample_idx})\\n'\n",
    "            f'Signals: {\", \".join([CHANNEL_MAPPING[s] for s in data_module.signal_keys])}',\n",
    "            fontsize=11,\n",
    "            fontweight='bold'\n",
    "        )\n",
    "        ax.set_xlabel('Time Step', fontsize=10)\n",
    "        ax.set_ylabel('Signal Value (Normalized)', fontsize=10)\n",
    "        ax.set_facecolor('#f8f8f8')\n",
    "        ax.grid(True, alpha=0.3, linewidth=0.5)\n",
    "\n",
    "    # Add colorbar with symmetric range for diverging colormap\n",
    "    norm = mcolors.Normalize(vmin=-1, vmax=1)\n",
    "    sm = plt.cm.ScalarMappable(cmap='icefire', norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=axes, orientation='vertical',\n",
    "                        pad=0.02, shrink=0.8)\n",
    "    cbar.set_label('Attribution (Blue=Negative, Red=Positive)',\n",
    "                   fontsize=10, fontweight='bold')\n",
    "    cbar.ax.tick_params(labelsize=9)\n",
    "\n",
    "    # Save the plot\n",
    "    filename_base = f\"attribution_colored_timeseries_{target_name.lower().replace(' ', '_')}\"\n",
    "    plt.savefig(plot_path / f\"{filename_base}.pdf\",\n",
    "                bbox_inches='tight', dpi=300)\n",
    "    plt.savefig(plot_path / f\"{filename_base}.png\",\n",
    "                bbox_inches='tight', dpi=150)\n",
    "    print(f\"Saved colored time series plot to {plot_path / filename_base}.pdf\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3518ab5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
